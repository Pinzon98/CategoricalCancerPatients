{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se instalan los paquetes necesarios para el proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zBKrne4hFa7l",
        "outputId": "08ba735e-4dea-44f6-a28c-32bd630980e5"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install inflect\n",
        "!pip install pandas-profiling==2.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Del paquete nltk, se obtienen las stopwords, el diccionario de palabras y el diccionario de palabras sinónimas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7djKpqIGue3",
        "outputId": "5f25cd7a-a2a7-48cf-a1dd-3038d6784ef5"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se importan las librerias a ser utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "aospPr11GzI3",
        "outputId": "11723237-15f8-4e35-e196-52fdea7e9ad3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import  accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgwp-UecIRC5"
      },
      "source": [
        "Se automatiza el proceso del procesamiento de datos y el modelado en el pipeline.\n",
        "\n",
        "Para el procesamiento de datos, se utiliza TfidfVectorizer para el vectorizado de los datos con los parametros, quita las palabras que no están en formato ASCII y tambien las stopwords inglesas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FStxR9FORMzV"
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline(\n",
        "  steps = [\n",
        "    (\"tfidf\", TfidfVectorizer(strip_accents=\"ascii\", stop_words=\"english\")),\n",
        "    (\"clf\", RandomForestClassifier(criterion='entropy', max_depth= 300, n_estimators=500))\n",
        "  ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se obtienen los datos del csb correspondiente y se proprocesa los labels para que sean de tipo categórico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('clinical_trials_on_cancer_data_clasificacion.csv', sep=',', encoding = 'utf-8')\n",
        "data_t = data\n",
        "data_t[\"label\"] = data_t[\"label\"].apply(lambda row: int(row.replace(\"__label__\",\"\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se dividen los datos en entrenamiento y prueba con un ratio de 2:8 respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g0as88ohRtbU"
      },
      "outputs": [],
      "source": [
        "objetivo = data_t[\"label\"]\n",
        "variables = data_t[\"study_and_condition\"]\n",
        "x_train, x_test, y_train, y_test = train_test_split(variables, objetivo, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se usa gridsearch para buscar los mejores parametros para el modelo. (y luego se le asigna el mejor modelo al pipeline de forma manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf',\n",
              "                 TfidfVectorizer(stop_words='english', strip_accents='ascii')),\n",
              "                ('clf',\n",
              "                 RandomForestClassifier(criterion='entropy', max_depth=300,\n",
              "                                        n_estimators=500))])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {\n",
        "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
        "    \"clf__max_depth\": [300, 1000],\n",
        "    \"clf__n_estimators\": [100, 500]\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "grid_search = GridSearchCV(pipe, parameters, n_jobs=-1, verbose=1)\n",
        "best_model = grid_search.fit(x_train, y_train)\n",
        "rfc_best = best_model.best_estimator_\n",
        "print(\"best params: \", best_model.best_params_)\n",
        "\"\"\"\n",
        "pipe.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se exporta el modelo como joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['modelo.joblib']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "dump(pipe, 'modelo.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "97cmWqF0SEUN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exactitud sobre entrenamiento: 1.00\n",
            "Exactitud sobre test: 0.80\n"
          ]
        }
      ],
      "source": [
        "y_pred_train = pipe.predict(x_train)\n",
        "y_pred_test = pipe.predict(x_test)\n",
        "print('Exactitud sobre entrenamiento: %.2f' % accuracy_score(y_train, y_pred_train))\n",
        "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se obtiene una exactitud sobre el conjunto de prueba de 0.80 y una exactitud de entrenamiento de 1.00."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(pipe.score(x_test, y_test))\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Proyecto1-etapa2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
